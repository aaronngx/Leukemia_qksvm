# Overfitting Prevention Analysis

## Executive Summary

**Overall Assessment:** âš ï¸ **MIXED** - Has good practices but also significant overfitting risks

**Risk Level:** ğŸŸ¡ **MODERATE-HIGH** (due to small dataset size and potential data leakage)

---

## âœ… Overfitting Prevention Mechanisms (GOOD PRACTICES)

### 1. **Independent Test Set (CRITICAL)**

```
Training Data:      38 patients (or 22 balanced)
                    â†“ Feature Selection
                    â†“ Model Training
                    âœ— NEVER seen by independent set

Independent Set:    34 patients (COMPLETELY SEPARATE)
                    â†“ ONLY used for final validation
                    âœ“ True unbiased performance
```

**Files:** `preprocessing.py:172-178`
```python
print("  Data Strategy (Golub Methodology):")
print("  â€¢ Gene selection: Selected patients ONLY (38 or 22)")
print("  â€¢ Internal validation: From selected patients")
print("  â€¢ Independent test: Completely separate (34 samples)")
```

âœ“ **Correctly implements Golub et al. (1999) methodology**
âœ“ **No data leakage to independent set** (when `use_all_data=False`)

---

### 2. **Cross-Validation Options**

**Available Strategies:**
```
â”œâ”€ 70/30 Train/Test Split (stratified)
â”œâ”€ 80/20 Train/Test Split (stratified)
â”œâ”€ 5-Fold Cross-Validation (stratified)
â”œâ”€ 10-Fold Cross-Validation (stratified)
â””â”€ LOOCV (Leave-One-Out CV) â† NEW
```

**Implementation:** `anova_f.py:187-242`, `signal_to_noise.py:337-397`

âœ“ **Stratified splitting** - maintains class balance
âœ“ **LOOCV available** - low bias for small datasets
âœ“ **Multiple validation options** - can assess robustness

---

### 3. **Regularization Mechanisms**

#### **SVM Regularization (C parameter)**

**Files:** `qksvm_golub.py:295`, `scad_svm.py:122`

```python
# QKSVM
clf = SVC(kernel="precomputed", C=C)  # Default C=1.0

# Controls margin vs misclassification trade-off
# Larger C â†’ Harder margin (more overfitting risk)
# Smaller C â†’ Softer margin (more regularization)
```

âœ“ **Tunable C parameter** - can adjust regularization strength
âš ï¸ **Default C=1.0** - not optimized per dataset

#### **SCAD Regularization**

**File:** `scad_svm.py:38-70`

```python
def scad_penalty(beta, lam, a=3.7):
    """
    SCAD penalty with three regions:
    1. |Î²| â‰¤ Î»: Linear penalty (like L1)
    2. Î» < |Î²| â‰¤ aÎ»: Quadratic transition
    3. |Î²| > aÎ»: Constant (no further shrinkage)
    """
    # Automatic feature selection via penalty
    # Less aggressive than L1 for large coefficients
```

âœ“ **Automatic feature selection** - reduces model complexity
âœ“ **Less aggressive than L1** - preserves important features
âœ“ **Auto-tuning available** - `auto_tune_lambda=True`

#### **StandardScaler Normalization**

**Files:** `amplitude_encoding.py:147-150`, `scad_svm.py:219-220`

```python
scaler = StandardScaler()  # Zero mean, unit variance
X_scaled = scaler.fit_transform(X_train)
```

âœ“ **Prevents feature dominance** - all features on same scale
âœ“ **Fit on training only** - no test set leakage

---

### 4. **Feature Selection Before Splitting**

**Files:** `anova_f.py:282-293`, `signal_to_noise.py:434-449`

```python
# CORRECT: Feature selection on training data only
X_train, y_train = load_training_data()  # 38 patients

# Feature selection uses ONLY training data
top_k_genes = select_features(X_train, y_train, k)

# THEN split for internal validation
X_train_int, X_test_int = split(X_train[top_k_genes])
```

âœ“ **No test set leakage** - feature selection on training only
âœ“ **Proper pipeline** - select â†’ split â†’ train

---

## âš ï¸ OVERFITTING RISKS (CONCERNS)

### 1. **Small Dataset Size (MAJOR RISK)**

```
Training: 22-38 patients
Features: 4-50 genes
Independent: 34 patients

Risk Level: ğŸ”´ HIGH
```

**Problems:**
- **High variance** - small sample size â†’ unstable estimates
- **Limited generalization** - may not represent population
- **Model complexity** - quantum models can overfit easily

**Evidence:**
```python
# Example: 22 patients with 16 genes
n_samples = 22
n_features = 16
ratio = n_samples / n_features = 1.375  # Very low!

# Rule of thumb: Need n_samples >> n_features
# Typically want ratio > 10
```

âš ï¸ **Recommendation:** Use LOOCV or 10-fold CV for small datasets

---

### 2. **`use_all_data` Flag (DATA LEAKAGE RISK)**

**Files:** `anova_f.py:255-293`, `signal_to_noise.py:406-449`

```python
def run_feature_selection(
    ...
    use_all_data: bool = False,  # âš ï¸ DANGER FLAG
):
    """
    If use_all_data=True:
        - Combines train + independent for feature selection
        - CREATES DATA LEAKAGE
        - Invalid for unbiased validation
    """
    if use_all_data and input_ind is not None:
        # âš ï¸ LEAKAGE: Independent set used for feature selection
        X_train = concat([X_train, X_ind])
        y_train = concat([y_train, y_ind])
```

ğŸ”´ **CRITICAL WARNING:**
- If `use_all_data=True`, independent set is "seen" during feature selection
- This creates **data leakage** - overly optimistic results
- Final accuracy on independent set is **biased**

**Current Default:** `use_all_data=False` âœ“ (Safe)

**In `preprocessing.py`:** âŒ **NOT EXPOSED TO USER**
- User cannot accidentally enable this flag
- Always defaults to `False` in interactive mode
- Only accessible via command-line argument

âœ“ **Good:** Default is safe
âš ï¸ **Risk:** Advanced users could enable via CLI

---

### 3. **No Hyperparameter Tuning by Default**

**Files:** `qksvm_golub.py:209-210`

```python
# Fixed hyperparameters (no tuning)
C: float = 1.0  # SVM regularization
```

âš ï¸ **Concerns:**
- Fixed C=1.0 may not be optimal
- No grid search or Bayesian optimization
- Quantum circuit depth/reps fixed
- Could be underfitting OR overfitting

**Mitigation:** ENSGA optimizer available
```python
# File: ensga_optimizer.py
# Multi-objective optimization of C and gamma
use_ensga=True  # Optimize hyperparameters
```

âœ“ **Available but not default**
âš ï¸ **Most users won't use it**

---

### 4. **VQC Model Complexity**

**File:** `vqc_golub.py:52-57`

```python
# VQC with TwoLocal ansatz
ansatz = TwoLocal(
    n_qubits=n_qubits,
    rotation_blocks=['rx', 'rz', 'rx'],  # 3 rotations per qubit
    entanglement='linear',
    reps=reps  # Number of repetitions
)

# Parameter count: n_qubits Ã— 3 Ã— (reps + 1)
# Example: 16 qubits, reps=2
# â†’ 16 Ã— 3 Ã— 3 = 144 trainable parameters

# Training samples: 22
# Parameters: 144
# Ratio: 0.15 (parameters >> samples) âš ï¸ OVERFITTING RISK
```

ğŸ”´ **HIGH RISK:** More parameters than samples

**Mitigation:**
- Amplitude encoding uses fewer qubits (log scaling)
  - 16 features â†’ 4 qubits â†’ 36 params (better ratio)
- Can reduce `reps` to decrease parameters

---

### 5. **No Early Stopping**

**File:** `vqc_golub.py` (training loop)

```python
# VQC training
optimizer = COBYLA(maxiter=50)

# âš ï¸ No validation loss monitoring
# âš ï¸ No early stopping
# âš ï¸ Always runs full 50 iterations
```

âš ï¸ **Could overfit** - no stopping criterion based on validation

---

### 6. **Feature Selection Instability**

**Small sample problem:**
```
With 22 patients:
- ANOVA F-test may be unstable
- SNR scores have high variance
- Different train/test splits â†’ different top genes
```

âš ï¸ **Recommendation:**
- Use ensemble feature selection
- Average rankings across multiple CV folds
- Check feature stability across splits

---

## ğŸ“Š Overfitting Risk Assessment by Configuration

| Configuration | Dataset Size | Feature/Sample Ratio | Risk Level | Recommendation |
|---------------|--------------|----------------------|------------|----------------|
| **22 patients, 4 genes, Amplitude** | Very Small | 4/22 = 0.18 | ğŸŸ¢ LOW | Good choice |
| **22 patients, 16 genes, Amplitude** | Very Small | 16/22 = 0.73 | ğŸŸ¡ MODERATE | Use LOOCV |
| **22 patients, 50 genes, Amplitude** | Very Small | 50/22 = 2.27 | ğŸ”´ HIGH | Too many features |
| **38 patients, 16 genes, Amplitude** | Small | 16/38 = 0.42 | ğŸŸ¡ MODERATE | Acceptable |
| **22 patients, 16 genes, Angle (VQC, reps=2)** | Very Small | 144/22 = 6.55 | ğŸ”´ CRITICAL | Severe overfitting risk |
| **38 patients, 16 genes, Angle (VQC, reps=1)** | Small | 96/38 = 2.53 | ğŸ”´ HIGH | Reduce reps or use QKSVM |

---

## ğŸ¯ Best Practices to Avoid Overfitting

### Recommended Configuration

```python
# GOOD CONFIGURATION
{
    "patients": 38,              # Use all available data
    "patient_balance": False,    # Don't reduce samples
    "genes": 16,                 # Reasonable feature count
    "gene_balance": True,        # Balanced selection
    "validation": "loocv",       # Exhaustive validation
    "encoding": "amplitude",     # Fewer qubits (log scaling)
    "classifier": "qksvm",       # No trainable parameters
    "use_all_data": False,       # âœ… NO DATA LEAKAGE
}
```

### Step-by-Step Recommendations

1. **Use Maximum Available Training Data**
   ```bash
   # Choose 38 patients (option 1)
   # Don't reduce to 22 unless class balance critical
   ```

2. **Conservative Feature Selection**
   ```bash
   # Start with k â‰¤ 16 genes
   # Ensure n_samples / k > 2
   ```

3. **Use LOOCV for Small Datasets**
   ```bash
   # Select option 2c: LOOCV
   # Provides low-bias estimates
   ```

4. **Prefer QKSVM over VQC**
   ```bash
   # QKSVM: No trainable parameters (just kernel)
   # VQC: Many trainable parameters (high overfitting risk)
   ```

5. **Use Amplitude Encoding**
   ```bash
   # 16 genes â†’ 4 qubits (log scaling)
   # vs 16 genes â†’ 16 qubits (linear scaling)
   ```

6. **Never Use `use_all_data=True`**
   ```bash
   # ALWAYS keep independent set separate
   # Only use for final validation
   ```

7. **Report Multiple Metrics**
   ```bash
   # Internal CV accuracy (potentially optimistic)
   # Independent set accuracy (TRUE performance)
   # If gap is large â†’ overfitting
   ```

---

## âœ… Validation Checklist

Before trusting results, verify:

- [ ] `use_all_data=False` in preprocessing
- [ ] Feature selection used ONLY training data
- [ ] Independent set NEVER used during:
  - [ ] Feature selection
  - [ ] Hyperparameter tuning
  - [ ] Model training
- [ ] Sample size reasonable: n_samples/n_features > 2
- [ ] Cross-validation used (not just single split)
- [ ] Results reported on independent set
- [ ] Performance gap checked: |CV_acc - Ind_acc| < 10%

---

## ğŸ“ˆ Detecting Overfitting

### Warning Signs

```python
# Example results:
Internal CV Accuracy:    95%  âœ“
Independent Accuracy:    65%  âš ï¸

Gap = 95% - 65% = 30%  ğŸ”´ SEVERE OVERFITTING
```

**Healthy Gap:** < 10%
**Moderate Gap:** 10-20%
**Severe Gap:** > 20%

### Common Causes

1. **Data leakage** - `use_all_data=True` was used
2. **Too many features** - k > n_samples/2
3. **Model too complex** - VQC with high reps
4. **Unstable feature selection** - different genes each fold
5. **Lucky train/test split** - use CV to verify

---

## ğŸ”¬ Comparison with Original Golub Study

| Aspect | Golub et al. (1999) | This Project | Assessment |
|--------|---------------------|--------------|------------|
| Training Size | 38 patients | 22-38 patients | âœ“ Same |
| Test Size | 34 patients (independent) | 34 patients | âœ“ Same |
| Feature Selection | P-score on train only | ANOVA/SNR on train only | âœ“ Correct |
| Validation | Independent set | Independent set + CV | âœ“ Better |
| Data Leakage Prevention | Yes | Yes (if use_all_data=False) | âœ“ Good |
| Model Complexity | Weighted voting (simple) | Quantum (complex) | âš ï¸ Higher risk |

---

## ğŸ“ Summary

### âœ… What Works Well

1. **Independent test set** properly isolated
2. **Multiple CV options** including LOOCV
3. **Regularization available** (SVM C, SCAD)
4. **Stratified splits** maintain class balance
5. **Default safe** - `use_all_data=False`

### âš ï¸ Areas of Concern

1. **Small dataset** (22-38 samples)
2. **High-dimensional risk** (k up to 50)
3. **VQC complexity** (many trainable parameters)
4. **No hyperparameter tuning** by default
5. **Potential data leakage** if `use_all_data=True` used

### ğŸ¯ Final Recommendation

**The project CAN avoid overfitting IF:**
- âœ… Use 38 patients (not 22)
- âœ… Keep k â‰¤ 16 genes
- âœ… Use LOOCV or 10-fold CV
- âœ… Prefer QKSVM over VQC
- âœ… Use amplitude encoding
- âœ… NEVER enable `use_all_data=True`
- âœ… Report independent set results
- âœ… Check internal vs independent gap

**Risk Level with Recommended Settings:** ğŸŸ¢ **LOW-MODERATE**

**Risk Level with Poor Settings:** ğŸ”´ **HIGH**

---

**Last Updated:** 2025-12-14
**Project:** Leukemia QKSVM Overfitting Analysis
